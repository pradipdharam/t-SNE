{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "**2. Mandatory Exercise**\n17.17 **Exercise: t-SNE visualization of Amazon reviews with polarity based color-coding**\nGet the Amaxon Food Reviews Data. Get the Bow, TF-IDF, Avg word2vec, TF-IDF weighted word2vec vector representations. Then do t-SNE visualization for each vector representation. Polarities for reviews are positive and negative, same should be visualized using t-SNE"
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "\"\"\"\n%matplotlib inline\n\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\nfrom nltk.stem.porter import PorterStemmer\n\n# using the SQLite Table to read data.\n#con = sqlite3.connect('./amazon-fine-food-reviews/database.sqlite') \n#con = sqlite3.connect('../input/database.sqlite') \ncon = sqlite3.connect('../input/amazon-fine-food-reviews/database.sqlite') \n\n#filtering only positive and negative reviews i.e. \n# not taking into consideration those reviews with Score=3\nfiltered_data = pd.read_sql_query(\"\"\"\n#### SELECT *\n#### FROM Reviews\n#### WHERE Score != 3\n\"\"\", con) \n\n# Give reviews with Score>3 a positive rating, and reviews with a score<3 a negative rating.\ndef partition(x):\n    if x < 3:\n        return 'negative'\n    return 'positive'\n\n#changing reviews with score less than 3 to be positive and vice-versa\nactualScore = filtered_data['Score']\npositiveNegative = actualScore.map(partition) \nfiltered_data['Score'] = positiveNegative\n\ndisplay= pd.read_sql_query(\"\"\"\n#### SELECT *\n#### FROM Reviews\n#### WHERE Score != 3 AND UserId=\"AR5J8UI46CURR\"\n#### ORDER BY ProductID\n\"\"\", con)\ndisplay\n\n#Sorting data according to ProductId in ascending order\nsorted_data=filtered_data.sort_values('ProductId', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')\n\n#Deduplication of entries\nfinal=sorted_data.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"}, keep='first', inplace=False)\nfinal.shape\n\n#Checking to see how much % of data still remains\n(final['Id'].size*1.0)/(filtered_data['Id'].size*1.0)*100\n\ndisplay= pd.read_sql_query(\"\"\"\n#### SELECT *\n#### FROM Reviews\n#### WHERE Score != 3 AND Id=44737 OR Id=64422\n#### ORDER BY ProductID\n\"\"\", con)\ndisplay\n\nfinal=final[final.HelpfulnessNumerator<=final.HelpfulnessDenominator]\n\n\"\"\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0949aefa59d25c023ceeef508c71b7b73f43f059"
      },
      "cell_type": "markdown",
      "source": "## Text Preprocessing: Stemming, stop-word removal and Lemmatization.\n\nNow that we have finished deduplication our data requires some preprocessing before we go on further with analysis and making the prediction model.\n\nHence in the Preprocessing phase we do the following in the order below:-\n\n1. Begin by removing the html tags\n2. Remove any punctuations or limited set of special characters like , or . or # etc.\n3. Check if the word is made up of english letters and is not alpha-numeric\n4. Check to see if the length of the word is greater than 2 (as it was researched that there is no adjective in 2-letters)\n5. Convert the word to lowercase\n6. Remove Stopwords\n7. Finally Snowball Stemming the word (it was obsereved to be better than Porter Stemming)<br>\n\nAfter which we collect the words used to describe positive and negative reviews"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4359c16c50c66a3f1ab8558b72a26bc70561f4be",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "\"\"\"\n\n# find sentences containing HTML tags\nimport re\ni=0;\nfor sent in final['Text'].values:\n    if (len(re.findall('<.*?>', sent))):\n        print(i)\n        print(sent)\n        break;\n    i += 1;    \n\nimport re\n# Tutorial about Python regular expressions: https://pymotw.com/2/re/\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nstop = set(stopwords.words('english')) #set of stopwords\nsno = nltk.stem.SnowballStemmer('english') #initialising the snowball stemmer\n\ndef cleanhtml(sentence): #function to clean the word of any html-tags\n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr, ' ', sentence)\n    return cleantext\ndef cleanpunc(sentence): #function to clean the word of any punctuation or special characters\n    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n    return  cleaned\n\n\n\"\"\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1023a7602a5e1fe7535ae77b10c7026a8ce34313",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "\"\"\"\n\n#Code for implementing step-by-step the checks mentioned in the pre-processing phase\n# this code takes a while to run as it needs to run on 500k sentences.\ni=0\nstr1=' '\nfinal_string=[]\nall_positive_words=[] # store words from +ve reviews here\nall_negative_words=[] # store words from -ve reviews here.\ns=''\nfor sent in final['Text'].values:\n    filtered_sentence=[]\n    #print(sent);\n    sent=cleanhtml(sent) # remove HTMl tags\n    for w in sent.split():\n        for cleaned_words in cleanpunc(w).split():\n            if((cleaned_words.isalpha()) & (len(cleaned_words)>2)):    \n                if(cleaned_words.lower() not in stop):\n                    s=(sno.stem(cleaned_words.lower())).encode('utf8')\n                    filtered_sentence.append(s)\n                    if (final['Score'].values)[i] == 'positive': \n                        all_positive_words.append(s) #list of all words used to describe positive reviews\n                    if(final['Score'].values)[i] == 'negative':\n                        all_negative_words.append(s) #list of all words used to describe negative reviews reviews\n                else:\n                    continue\n            else:\n                continue \n    #print(filtered_sentence)\n    str1 = b\" \".join(filtered_sentence) #final string of cleaned words\n    #print(\"***********************************************************************\")\n    \n    final_string.append(str1)\n    i+=1\n    \n    \"\"\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c46c2d75e8660a88bda1b1d98744ec1e0a4080d5",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "\"\"\"\n\nfinal['CleanedText']=final_string #adding a column of CleanedText which displays the data after pre-processing of the review \ncleanedText=final['CleanedText']\nfinalScore = final['Score']\nprint(finalScore.head(2))\nprint(cleanedText.head(2))\nprint(\"Shape cleanedText: \",cleanedText.shape)\nprint(\"Shape: finalScore\",finalScore.shape)\n\nfinal1=pd.DataFrame()\nfinal1['Text'] = final['Text']\nfinal1['CleanedText']  = final['CleanedText']\nfinal1['Score'] = finalScore\n\nfinal1.to_csv('final1.csv')\n\n\"\"\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "efc1a166f364ece947bd5ac2beb41babce8669f8",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "%matplotlib inline\n\nfrom sklearn.manifold import TSNE\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\nfrom nltk.stem.porter import PorterStemmer\n\nfinal1=pd.read_csv(\"../input/final1/final1.csv\")\nText = final1['Text']\ncleanedText  = final1['CleanedText']\nfinalScore = final1['Score']\n\ndef tsne_visualize(data, labels):\n  #model = TSNE(n_components=2, random_state=0)\n  model = TSNE(n_components=2, random_state=0, perplexity = 50, n_iter=5000)\n  # configuring the parameteres,  # the number of components = 2,  # default perplexity = 30,  # default learning rate = 200\n  # default Maximum number of iterations for the optimization = 1000\n  tsne_data = model.fit_transform(data)  # creating a new data frame which help us in ploting the result data\n  tsne_data = np.vstack((tsne_data.T, labels)).T\n  tsne_df = pd.DataFrame(data=tsne_data, columns=(\"Dim_1\", \"Dim_2\", \"label\"))\n  # Ploting the result of tsne\n  sns.FacetGrid(tsne_df, hue=\"label\", size=6).map(plt.scatter, 'Dim_1', 'Dim_2').add_legend()\n  plt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "230a42ce8c61f1526ff0a6362ef01e7700d4fce4"
      },
      "cell_type": "markdown",
      "source": "############################################################################################"
    },
    {
      "metadata": {
        "_uuid": "e4de7b5f3ca8a7c82a8ee96dc1d693274b279429"
      },
      "cell_type": "markdown",
      "source": "**BAG OF WORDS**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2f6181883f8805b7c137c18cfefc460a7329a54a",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "count_vect = CountVectorizer() #in scikit-learn\nfinal_counts_bow = count_vect.fit_transform(cleanedText.values)\nfinal_counts_bow.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "41dc7b0caa5937102ba60512ba88b6bc4d32a2a3",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "type(final_counts_bow)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "eab7a440e3c6c00339ca4d90ad47988eec277c37",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "final_counts_bow",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "84b6b12f15bc64838694cc666cc0ec2d5cde67f8"
      },
      "cell_type": "markdown",
      "source": "Processing only 5000 records since entire dataset processing takes too much RAM"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ceaf512f6342bc6bae3718e261dd559d593ab570",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "ndatapoints=5000\ndata = final_counts_bow[0:ndatapoints,:]\nlabels = finalScore[0:ndatapoints]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "131c75e6e5e7d7f9ea583a3db95ec562c61fed1a"
      },
      "cell_type": "markdown",
      "source": "* Converting sparce to dense metrics to standardize the data\n* Standardizing the data"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6238e1289d12a7e864dbbf15d7ef1fa1e15a19a9",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Converting sparce to dense metrics to standardize the data\ndata_dense=data.todense()\n\n# Standardizing the data\nfrom sklearn.preprocessing import StandardScaler\nstandardized_data = StandardScaler().fit_transform(data_dense)\nstandardized_data.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "462201f0f50b5eab788a4497deec5decce78e162"
      },
      "cell_type": "markdown",
      "source": "**t-SNE visualization for BAG OF WORDS**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "602785bee654f1aab34e8f9da0e2fc3f532ba4d4",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# TSNE\ntsne_visualize(standardized_data,labels)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "86e4def2d7ab96f4e8d5f0239e05a09ab0b61dfe"
      },
      "cell_type": "markdown",
      "source": "############################################################################################"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4fb16f55362fe7324f844e8d9bd530d4f51885a7"
      },
      "cell_type": "markdown",
      "source": "**TERM FREQUENCY - INVERSE DOCUMENT FREQUENCY : TF-IDF**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0027d74ce050cebb8a2597caf5784d313dfaee5d",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "tf_idf_vect = TfidfVectorizer()\nfinal_tf_idf = tf_idf_vect.fit_transform(cleanedText.values)\nfinal_tf_idf.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e6ba0e6aa2bdceeac42c37278a7fc43a155ba39c"
      },
      "cell_type": "markdown",
      "source": "Processing only 5000 records since entire dataset processing takes too much RAM"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "153dd160cb76bda3c34d92d8f7b793d42a97fb1c",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "ndatapoints=5000\ndata = final_tf_idf[0:ndatapoints,:]\nlabels = finalScore[0:ndatapoints]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "11bd2d5779f18d84adfdf9d707403ee1b62ae607"
      },
      "cell_type": "markdown",
      "source": "* Converting sparce to dense metrics to standardize the data\n* Standardizing the data"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "fc1e126b2f120859b0ddb47f87d2d3a62b628b5d"
      },
      "cell_type": "code",
      "source": "# Converting sparce to dense metrics to standardize the data\ndata_dense=data.todense()\n\n#Standardizing the data\nfrom sklearn.preprocessing import StandardScaler\nstandardized_data = StandardScaler().fit_transform(data_dense)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b21eb1aa92a2e680bfd2c5429e33a82060ef291a"
      },
      "cell_type": "markdown",
      "source": "**t-SNE visualization for TF-IDF**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8d2590bc412c9409d9fc24453edb7580d99ff292",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "tsne_visualize(standardized_data,labels)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0143bacfca9cf17f6c114d2c8bec4642f08bce02"
      },
      "cell_type": "markdown",
      "source": "############################################################################################"
    },
    {
      "metadata": {
        "_uuid": "6c1fd8694294da94f346181e9b4f05f95c217f23"
      },
      "cell_type": "markdown",
      "source": "**OBSERVATIONS:**\n* **t-SNE visualization looks almost similar for both BoW and TF-IDF**"
    },
    {
      "metadata": {
        "_uuid": "a7985d8d70f2b456409caaf584d10fafc5f883fc"
      },
      "cell_type": "markdown",
      "source": "############################################################################################"
    },
    {
      "metadata": {
        "_uuid": "7567d48232b301e36c2f6fc0ec586d95b859c733"
      },
      "cell_type": "markdown",
      "source": "**I'm using google word to vector to measure semantic similarities**"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "b3633647f9c9766a0c0e18fbcfa99db6cc26a54b"
      },
      "cell_type": "markdown",
      "source": "**Word2Vec**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3979c8e653d7c410f61e8b2351debc5d60b006e1",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#\"\"\"\n# Using Google News Word2Vectors\nfrom gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors\nimport pickle\nmodel = KeyedVectors.load_word2vec_format('../input/googlenews-vectors-negative300.bin/GoogleNews-vectors-negative300.bin', binary=True)\n#model.wv['computer']; #print(model.wv.similarity('woman', 'man')); #print(model.wv.similarity('queen', 'queen'))\n#print(model.wv.most_similar('tasty') ) # \"tasti\" is the stemmed word for tasty, tastful\n#\"\"\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "96e9b7f76415d01dcbc1d76d1d83166d25a5dcf9"
      },
      "cell_type": "markdown",
      "source": "############################################################################################"
    },
    {
      "metadata": {
        "_uuid": "7a91a539ac19bd9829115f2225df10cb8d078ce5"
      },
      "cell_type": "markdown",
      "source": "**Average Word2Vec**"
    },
    {
      "metadata": {
        "_uuid": "6b26e17af82b59b8ae1b1283a988e2414b3dd0e4"
      },
      "cell_type": "markdown",
      "source": "Processing only 5000 records since entire dataset processing takes too much RAM"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "436b10bb7fa6bebe377cc9d1128bc2e598581b32",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "ndatapoints=5000\ndata = cleanedText[0:ndatapoints]\nlabels = finalScore[0:ndatapoints]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "160e0993abd26639d5965df20f8e21a1b46d1d84"
      },
      "cell_type": "markdown",
      "source": "Getting vector for each word in review and creating new scentense vector such that its an average of word2vector of all the words in review text"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "eb013c3466c2f7b344df0df926d9723913cd9001",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# average Word2Vec\n# compute average word2vec for each review.\nsent_vectors = []; # the avg-w2v for each sentence/review is stored in this list\nfor sent in data: # for each review/sentence\n    #print(\"Sentense : \", sent, \"Type: \",type(sent))\n    sent_vec = np.zeros(300) # as word vectors are of zero length\n    cnt_words =0; # num of words with a valid vector in the sentence/review\n    #for word in str(sent).split():\n        #print(\"word: \", str(word))\n    for word in str(sent).split(): # for each word in a review/sentence\n        try:\n            #print(\"word : \", word)\n            vec = model.wv[word]\n            #print(\"vec : \", vec.shape)\n            sent_vec += vec\n            cnt_words += 1\n        except:\n            #print(\"Exception\")\n            pass\n    sent_vec /= cnt_words\n    sent_vectors.append(sent_vec)\nprint(len(sent_vectors))\nprint(len(sent_vectors[0]))\n#print(\"Shape: \",sent_vectors.shape)\n#\"\"\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ae8ecee5c30f696d0fbe6df083c84ee7ccd53933"
      },
      "cell_type": "markdown",
      "source": "**t-SNE visualization for average word2vector**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e094bb80caaf764551a51afe239f4f23ad5dbd5f",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "tsne_visualize(sent_vectors, labels)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2b8485fde6e4ba5fa4914004bbeaa6c45c3114ac"
      },
      "cell_type": "markdown",
      "source": "############################################################################################"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "d4539450fbe4bb5d39b60dfefd875fd7fae928a9"
      },
      "cell_type": "markdown",
      "source": "**TF-IDF weighted word2vector**"
    },
    {
      "metadata": {
        "_uuid": "1a816aa9341e3912077d946562add8caf4255164"
      },
      "cell_type": "markdown",
      "source": "Processing only 5000 records since entire dataset processing takes too much RAM"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "84ec7ffbef9ed8d4a5c979baff08ae56faa4422b"
      },
      "cell_type": "code",
      "source": "ndatapoints=5000\ndata = cleanedText[0:ndatapoints]\nlabels = finalScore[0:ndatapoints]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "90b772f360701ec4513673c5181e7e14f6caaf1b"
      },
      "cell_type": "markdown",
      "source": "Getting vector for each word in review and creating new scentense vector such that word2vector is been weighted with respective TF-IDF of the review"
    },
    {
      "metadata": {
        "_uuid": "36ac7c7779c94d7e9a38ca2648e0fc9be90be95f"
      },
      "cell_type": "markdown",
      "source": "I have commented the print statements which I did added to undestand the execution throughly for few data points"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9ef57c18f8c002ccb9f956db8ab49ff4bdc5b32a",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# TF-IDF weighted Word2Vec\ntfidf_feat = tf_idf_vect.get_feature_names() # tfidf words/col-names\n# final_tf_idf is the sparse matrix with row= sentence, col=word and cell_val = tfidf\n\ntfidf_sent_vectors = []; # the tfidf-w2v for each sentence/review is stored in this list\nrow=0;\nfor sent in data: # for each review/sentence\n    sent_vec = np.zeros(300) # as word vectors are of zero length\n    weight_sum =0; # num of words with a valid vector in the sentence/review\n    #sent='littl book make son laugh loud recit car drive along alway sing refrain hes learn whale india droop love new word book introduc silli classic book will bet son still abl recit memori colleg'\n    #print(\"Sentense : \", sent)\n    for word in sent.split(): # for each word in a review/sentence\n        try:\n            #print(\"Word: \", word)\n            vec = model.wv[word]\n            #print(\"Vector: \", vec, \" Size: \", vec.shape)\n            # obtain the tf_idfidf of a word in a sentence/review\n            tf_idf = final_tf_idf[row, tfidf_feat.index(word)]\n            #print(\"tfidf: \", tf_idf)\n            sent_vec += (vec * tf_idf)\n            #print(\"sent_vec Calculated\")\n            weight_sum += tf_idf\n        except:\n           # print(\"Exception\")\n            #e = sys.exc_info()[0]\n            #print(\"Exception: \",e)\n            ##write_to_page( \"<p>Error: %s</p>\" % e )\n            pass\n    zero_weight_sum_count=0\n    if(weight_sum != 0):\n        sent_vec /= weight_sum\n    else        :\n        zero_weight_sum_count += 1\n    tfidf_sent_vectors.append(sent_vec)\n    row += 1\n    \nprint(len(tfidf_sent_vectors))\nprint(len(tfidf_sent_vectors[0]))   \n#print(\"zero_weight_sum_count: \", zero_weight_sum_count)\n\n#print(\"tfidf_sent_vectors : \")\n#print(tfidf_sent_vectors)\n#for vec in tfidf_sent_vectors:\n #   for i in vec:\n #       if(i!=0):\n #           print(\"i=\",i)        ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a82f4062c7791fcc9b29ddb100e4fe105fb630bd"
      },
      "cell_type": "markdown",
      "source": "**t-SNE visualization for TF-IDF weighted word2vector**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3c953d43b399ae81bebbc063e6aa595e8925182e",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# TSNE\ntsne_visualize(tfidf_sent_vectors, labels)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "01c05fb9c5015aa9cfbbd7bfd66f6d59b3e98b5a"
      },
      "cell_type": "markdown",
      "source": "############################################################################################"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "6395c615806a4d4896c269fcb68b694c11600022"
      },
      "cell_type": "markdown",
      "source": "**OBSERVATIONS:**\n* **t-SNE visualization looks almost similar for both Avg word2vec and TF-IDF weighted word2vec**\n* **t-SNE visualization for both BoW and TF-IDF looks different than both Avg word2vec and TF-IDF weighted word2vec**"
    },
    {
      "metadata": {
        "_uuid": "b13cda7e22c7a7bacf6e248d7b248330dd695626"
      },
      "cell_type": "markdown",
      "source": "############################################################################################\n"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "a04100281a16dd4589be94d3c9ae500e9c497a38"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "be09cbfa51b732ec798090cc21b0b6ecbaae8d29"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "9a3199c990a85b05ecbb6819ada420dbb50cb6ec"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}